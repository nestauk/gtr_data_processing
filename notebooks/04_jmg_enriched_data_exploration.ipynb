{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GtR enriched exploration\n",
    "\n",
    "In this notebook we explore the results of the enrichment of the GtR data using three sets of labels:\n",
    "\n",
    "* Academic disciplines, based on a model trained on a labelled subset of the GtR data\n",
    "* Industries, based on a model trained on a corpus of business website data\n",
    "* SDGs based on a labelled corpus of SDG related documents.\n",
    "\n",
    "We will load the data, perform an analysis of salient terms, explore correlations between enriched variables and with other metadata available etc.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string as st\n",
    "import wordcloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put functions and things here\n",
    "\n",
    "def get_latest_file(date_list,date_format='%d-%m-%Y'):\n",
    "    '''\n",
    "    This function takes a list of date strings and returns the most recent one\n",
    "    \n",
    "    Args:\n",
    "        date_list: a list of strings with the format date_filename\n",
    "        date_format: the format for the date, defaults to %d-%m-%Y\n",
    "    \n",
    "    Returns:\n",
    "        The element with the latest date\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #This gets the maximum date in the gtr directory\n",
    "    dates = [datetime.datetime.strptime('-'.join(x.split('_')[:3]),date_format) for x in date_list]\n",
    "    \n",
    "    #Return the most recent file\n",
    "    most_recent = sorted([(x,y) for x,y in zip(date_list,dates)],key=lambda x:x[1])[-1][0]\n",
    "    \n",
    "    return(most_recent)\n",
    "                                   \n",
    "\n",
    "def flatten_list(my_list):\n",
    "    '''\n",
    "    Flattens a list\n",
    "    '''\n",
    "    \n",
    "    return([x for el in my_list for x in el])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalientWords():\n",
    "    '''\n",
    "    Class that extracts salient words from clusters of data.\n",
    "    \n",
    "    Arguments:\n",
    "        A dataframe and two strings (the variable to groupby and the variable to use as text)\n",
    "        \n",
    "    Methods:\n",
    "        .count_vect(): word frequencies for all words (takes **kwargs for additional parameters in the count vectorisation)\n",
    "        .salient(): tfidf. It will also take **kwargs and a threshold for including words in the results\n",
    "        .visualise(): it visualises the data as wordclouds\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,df,categories,text):\n",
    "        '''\n",
    "        Initialises with key variables\n",
    "        \n",
    "        '''    \n",
    "        \n",
    "        \n",
    "        #This creates the joined corpus\n",
    "        self.grouped_corpus = df.groupby(categories)[text].apply(lambda x: ' '.join(x))\n",
    "        \n",
    "        \n",
    "        #Remove digits and special \n",
    "        dig = r'|'.join(st.digits)\n",
    "        out = '\\W+'\n",
    "        \n",
    "        self.processed_text = [re.sub(out,' ',re.sub(dig,' ',x.lower())) for x in self.grouped_corpus]\n",
    "        \n",
    "        #This is a dict we will use to store the results later\n",
    "        self.groups = {i:[] for i in self.grouped_corpus.index}\n",
    "        \n",
    "        #return(self)\n",
    "        \n",
    "    def word_freqs(self,**kwargs):\n",
    "        '''\n",
    "        Terms frequencies over categories\n",
    "        \n",
    "        '''\n",
    "        #load corpus\n",
    "        X = self.processed_text\n",
    "        \n",
    "        count_vect = CountVectorizer(**kwargs)\n",
    "        \n",
    "        #Store outputs\n",
    "        self.count_vect = count_vect\n",
    "        self.token_freqs = count_vect.fit_transform(X)\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    def salient(self,min_threshold=1000,extra_stops=['research','project','new','projects'],**kwargs):\n",
    "        '''\n",
    "        Salient terms in the data.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Get selected words\n",
    "        \n",
    "        word_freqs = pd.DataFrame(self.token_freqs.todense(),columns=self.count_vect.get_feature_names())\n",
    "        \n",
    "        word_freqs_total = word_freqs.sum(axis=0)\n",
    "        \n",
    "        #Create a dict so we can put word frequencies together with salient words later\n",
    "        #word_freqs_dict = word_freqs.to_dict()\n",
    "        \n",
    "        #I am interested in any words above the threshold\n",
    "        my_words = [x for x in word_freqs_total.index[word_freqs_total>min_threshold] if x not in extra_stops]\n",
    "        \n",
    "        \n",
    "        #Initialise the tfidf\n",
    "        tf = TfidfTransformer(**kwargs)\n",
    "        \n",
    "        \n",
    "        #out\n",
    "        X = tf.fit_transform(self.token_freqs)\n",
    "        \n",
    "        X_selected = pd.DataFrame(X.todense(),columns=self.count_vect.get_feature_names())[my_words]\n",
    "            \n",
    "            \n",
    "        #Store the results\n",
    "        for n,x in enumerate(self.groups.keys()):\n",
    "            \n",
    "            #Creates the dataframe combining tfs and wfs\n",
    "            result = pd.concat([X_selected.iloc[n],word_freqs.iloc[n][my_words]],axis=1)\n",
    "            \n",
    "            result.columns = ['tfidf','count'] \n",
    "                   \n",
    "            self.groups[x] = result\n",
    "            \n",
    "        return(self)\n",
    "        \n",
    "    def get_summary(self,tf_threshold=90,wf_threshold=75):\n",
    "        '''\n",
    "        \n",
    "        Extracts a summary of the data based on tf and wf thresholds\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.summary={i:[] for i in self.groups.keys()}\n",
    "        \n",
    "        for x in self.groups.keys():\n",
    "            \n",
    "            #Creates the dataframe\n",
    "            result = self.groups[x]\n",
    "            \n",
    "            tf_thres = np.percentile(result['tfidf'],tf_threshold)\n",
    "            \n",
    "            summary = result.loc[result['tfidf']>tf_thres]\n",
    "            \n",
    "            wf_thres = np.percentile(result['count'],wf_threshold)\n",
    "            \n",
    "            summary_2 = summary.loc[summary['count']>wf_thres]\n",
    "                   \n",
    "            self.summary[x] = summary_2.sort_values('tfidf',ascending=False)\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "def make_wordcloud(term_freqs_df,var,name,ax):\n",
    "    '''\n",
    "    This function takes a df generated by the SalientWords class and returns a wordcloud\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    input_dict = {w:f for w,f in zip(term_freqs_df.index,term_freqs_df[var])}\n",
    "\n",
    "    wc = wordcloud.WordCloud(background_color=\"black\").generate_from_frequencies(input_dict)\n",
    "\n",
    "    ax.imshow(wc)\n",
    "    \n",
    "    ax.axis('off')\n",
    "    \n",
    "    ax.set_title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that this also includes the discipline predictions\n",
    "\n",
    "sector_df = pd.read_csv('../data/processed/21_5_2019_gtr_with_industry_labels.csv',compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_df = pd.read_csv('../data/processed/2_5_2019_gtr_sdg_labelled.csv',compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine both df avoding repeated columns\n",
    "combined = pd.concat([sector_df,sdg_df[[x for x in sdg_df.columns if x not in sector_df.columns]]],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lists of category elements for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_list = [x for x in combined.columns if 'disc_' in x]\n",
    "industry_list = [x for x in combined.columns if any(sect in x for sect in ['primary','construction','manufacture','services'])]\n",
    "sdg_list = [x for x in combined.columns if 'sdg_' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.columns = ['ind_'+x if x in industry_list else x for x in combined.columns]\n",
    "\n",
    "industry_list = ['ind_'+x for x in industry_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disciplines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis\n",
    "\n",
    "def prediction_diagnostics(df,predicted_list,ax,prob_threshold=0.05):\n",
    "    '''\n",
    "    This function runs a bunch of tests with the vectors of predictions we have loaded.\n",
    "    \n",
    "    This includes:\n",
    "    \n",
    "    \n",
    "    -Create a df with the variables.\n",
    "    -Remove noisy predictions (below threshold)\n",
    "    -calculate the variance of predictions per observation and plot it.\n",
    "    -label predictions which are tight (in the top quartile of variance for each category)\n",
    "    -calculate kurtosis and label observations with high curtosis\n",
    "    -label predictions which are in the top quartile for the total and a category\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Remove probabilities below threshold\n",
    "    my_df = df[predicted_list].copy().applymap(lambda x: 0 if x<prob_threshold else x)\n",
    "    \n",
    "    #Diagnostics df\n",
    "    diag_df = pd.DataFrame()\n",
    "    \n",
    "    #Maximum prediction\n",
    "    \n",
    "    diag_df['max_pred'] = my_df.max(axis=1)\n",
    "    \n",
    "    #top category in obs\n",
    "    diag_df['top_category'] = my_df.idxmax(axis=1)\n",
    "    \n",
    "    #Calculate variance\n",
    "    diag_df['prediction_variance'] = my_df[predicted_list].apply(lambda x: np.var(x),axis=1)\n",
    "\n",
    "    diag_df.groupby('top_category')['prediction_variance'].mean().sort_values(ascending=False).plot.bar(title='Mean variance in predictins by top category',color='blue',\n",
    "                                                                                                                           ax=ax[0])\n",
    "\n",
    "    \n",
    "    #Is a variable in the prediction quartile\n",
    "    pred_variance_quartile = diag_df.groupby('top_category')['prediction_variance'].apply(lambda x: np.percentile(x,75))\n",
    "    \n",
    "    pred_variance_quartile_all = np.percentile(diag_df['prediction_variance'],75)\n",
    "    \n",
    "    diag_df['tight_prediction']= [x>pred_variance_quartile[cat] for x,cat in zip(diag_df['prediction_variance'],diag_df['top_category'])]\n",
    "    \n",
    "    diag_df['tight_prediction_all']= [x>pred_variance_quartile_all for x in diag_df['prediction_variance']]\n",
    "    \n",
    "    pd.crosstab(diag_df['top_category'],diag_df['tight_prediction_all'],normalize=1).plot.bar(title='Tight predictions by category',ax=ax[1])\n",
    "    \n",
    "    \n",
    "    #Kurtosis\n",
    "    \n",
    "    diag_df['kurtosis'] = my_df.apply(lambda x: kurtosis(x),axis=1)\n",
    "    \n",
    "    #Kurtosis plot\n",
    "    \n",
    "    diag_df.groupby('top_category')['kurtosis'].mean().sort_values(ascending=False).plot.bar(color='blue',title='Mean kurtosis by top category',ax=ax[2])\n",
    "    \n",
    "    \n",
    "    #Percentiles in predicted values\n",
    "    pc_75_preds_all = np.percentile(flatten_list([my_df.loc[my_df[cat]>0,cat] for cat in predicted_list]),95)\n",
    "\n",
    "    #Are any of the predictions for a project above the 75 pc for all predictions?\n",
    "    diag_df['has_top_pred']= my_df[predicted_list].apply(lambda x: any(v>pc_75_preds_all for v in x),axis=1)\n",
    "    \n",
    "    #Prediction percentile per sector\n",
    "    pc_75_by_cat = {cat: np.percentile(my_df.loc[my_df[cat]>0,cat],95) for cat in predicted_list}\n",
    "\n",
    "    pd.DataFrame(pc_75_by_cat,index=[0]).T.sort_values(0,ascending=False).plot.bar(color='blue',legend=False,title='75 pc probability',ax=ax[3])\n",
    "    \n",
    "    df_quarts = pd.DataFrame()\n",
    "    \n",
    "    for cat in predicted_list:\n",
    "        df_quarts[cat+'_top_q'] = my_df[cat]>pc_75_by_cat[cat]\n",
    "    \n",
    "    return([diag_df,df_quarts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(7,15),nrows=4)\n",
    "\n",
    "disc_diag = prediction_diagnostics(combined,disc_list,prob_threshold=0.1,ax=ax)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['disc_top'] = disc_diag[0]['top_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_disc = SalientWords(combined,categories='disc_top',text='abstract')\n",
    "sal_disc.word_freqs(**{'stop_words':'english','max_features':2000,'ngram_range':(1,2)}).salient(min_threshold=500).get_summary(wf_threshold=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=2,nrows=4,figsize=(10,10))\n",
    "\n",
    "for n,name in enumerate(sal_disc.summary.keys()):\n",
    "    \n",
    "    #print(n)\n",
    "    \n",
    "    if n<4:\n",
    "        make_wordcloud(sal_disc.summary[name],'tfidf',name,ax=ax[n][0])\n",
    "        \n",
    "    else:\n",
    "        make_wordcloud(sal_disc.summary[name],'tfidf',name,ax=ax[n-4][1])\n",
    "        \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Industries\n",
    "\n",
    "#### Looking for tight predictions\n",
    "\n",
    "We are particularly interested in predictions that are 'tight' (ie the distribution is highly skewed) and confident (they have high values)\n",
    "\n",
    "We do this a couple of ways\n",
    "\n",
    "1. Calculate variance in prediction for each observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(25,30),nrows=4)\n",
    "\n",
    "ind_diag = prediction_diagnostics(combined,industry_list,prob_threshold=0,ax=ax)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove some sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After some manual checking, we remove the below. They tend to misclassify projects for a variety of reasons potentially linked to noise in the source data\n",
    "\n",
    "sectors_remove = ['services_consumer_retail','services_education_post_primary','services_travelling','services_real_state','services_administrative',\n",
    "                 'services_electronics_machinery','primary_fishing','services_textiles']\n",
    "\n",
    "industry_selected = [x for x in industry_list if x[4:] not in sectors_remove]\n",
    "\n",
    "#gtr_w_industries['top_industry_2'] = gtr_w_industries[industry_selected].idxmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['ind_top'] = combined[industry_selected].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined industry prediction only considering predictions in the top 75pc for a sector\n",
    "#This is very slow!\n",
    "combined['ind_top_2'] = [row[1][industry_selected].astype('float64').idxmax() if \n",
    "                         ind[1][[x+'_top_q' for x in industry_selected]].sum()>0 else np.nan for row,ind in zip(combined.iterrows(),ind_diag[1].iterrows())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([combined['ind_top'].value_counts(),combined['ind_top_2'].value_counts()],axis=1).plot.bar(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_ind = SalientWords(combined,categories='ind_top_2',text='abstract')\n",
    "sal_ind.word_freqs(**{'stop_words':'english','max_features':2000,'ngram_range':(1,2)}).salient(min_threshold=500).get_summary(wf_threshold=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_rows = int(len(set(combined['ind_top_2']))/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=2,nrows=cat_rows,figsize=(10,50))\n",
    "\n",
    "for n,name in enumerate(sal_ind.summary.keys()):\n",
    "    \n",
    "    #print(n)\n",
    "    \n",
    "    if n<cat_rows:\n",
    "        make_wordcloud(sal_ind.summary[name],'tfidf',name,ax=ax[n][0])\n",
    "        \n",
    "    else:\n",
    "        make_wordcloud(sal_ind.summary[name],'tfidf',name,ax=ax[n-cat_rows][1])\n",
    "        \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,20),nrows=4)\n",
    "\n",
    "sdg_diag = prediction_diagnostics(combined,sdg_list,prob_threshold=0,ax=ax)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_final = [s for s in sdg_list if 'reduced_inequality' not in s]\n",
    "\n",
    "combined['top_sdg'] =  combined[sdg_final].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined industry prediction only considering predictions in the top 75pc for a sector\n",
    "#This is very slow!\n",
    "combined['sdg_top_2'] = [row[1][sdg_final].astype('float64').idxmax() if \n",
    "                         ind[1][[x+'_top_q' for x in sdg_final]].sum()>0 else np.nan for row,ind in zip(combined.iterrows(),sdg_diag[1].iterrows())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_sdg = SalientWords(combined,categories='sdg_top_2',text='abstract')\n",
    "sal_sdg.word_freqs(**{'stop_words':'english','max_features':20000,'ngram_range':(1,2)}).salient(min_threshold=500).get_summary(wf_threshold=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=2,nrows=7,figsize=(10,20))\n",
    "\n",
    "for n,name in enumerate(sal_sdg.summary.keys()):\n",
    "    \n",
    "    #print(n)\n",
    "    \n",
    "    if n<7:\n",
    "        make_wordcloud(sal_sdg.summary[name],'tfidf',name,ax=ax[n][0])\n",
    "        \n",
    "    else:\n",
    "        make_wordcloud(sal_sdg.summary[name],'tfidf',name,ax=ax[n-7][1])\n",
    "        \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('/Users/jmateosgarcia/Desktop/sdg_salient.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Focus the analysis on projects between 2006 and 2018.\n",
    "\n",
    "df = combined.loc[(combined.year>=2006) & (combined.year<2019)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disciplines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.disc_top.value_counts().plot.bar(color='blue')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f'../reports/figures/temp_scotland_living_doc/{today_str}_research_discipline_counts.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_sorted = df.disc_top.value_counts().index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To which extent is the importance of engineering and technology driven by Innovate UK?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*pd.crosstab(df.disc_top,df.funder,normalize=1)['Innovate UK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['year'],df['disc_top']).plot(figsize=(10,5))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f'../reports/figures/temp_scotland_living_doc/{today_str}_research_discipline_trends.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['year'],df['funder']).plot(figsize=(10,5))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f'../reports/figures/temp_scotland_living_doc/{today_str}_research_funder_trends.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers of social science research projects look low. Could it be that they tend to be more interdisciplinary / receive lower probabilities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_preds_social,threshold_preds_eng = [pd.concat([df.loc[df[d]>thr].year.value_counts(normalize=False) for thr in np.arange(0.1,1,0.25)],axis=1) for d in \n",
    "                                              ['disc_social','disc_eng_tech']]\n",
    "\n",
    "threshold_preds_social.columns = ['p > '+str(np.round(x,2)) for x in np.arange(0.1,1,0.25)]\n",
    "threshold_preds_eng.columns = ['p > '+str(np.round(x,2)) for x in np.arange(0.1,1,0.25)]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(6,6),nrows=2,sharex=True)\n",
    "\n",
    "threshold_preds_social.rolling(window=3).mean().plot(ax=ax[0])\n",
    "threshold_preds_eng.rolling(window=3).mean().plot(ax=ax[1])\n",
    "\n",
    "ax[0].set_title('Social Sciences')\n",
    "ax[1].set_title('Engineering and technology')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f'../reports/figures/temp_scotland_living_doc/{today_str}_social_science_comp.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*df.ind_top_2.isnull().sum()/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have removed around 3.6% of projects because they didn't have strong predictions in any categories. We could be more strict.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_path = '../reports/figures/temp_scotland_living_doc/'\n",
    "\n",
    "\n",
    "def save_today(name,path=report_path,today_str=today_str):\n",
    "    \n",
    "    plt.savefig(path+today_str+'_'+name+'.pdf')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Industry frequencies\n",
    "df.ind_top_2.value_counts().plot.bar(figsize=(10,5),color='blue')\n",
    "plt.tight_layout()\n",
    "\n",
    "save_today('industry_counts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industries_sorted = df.ind_top_2.value_counts().index\n",
    "funders_sorted = df.funder.value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Who funds what?\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "pd.crosstab(df.ind_top_2,df.funder,normalize=0).loc[industries_sorted,funders_sorted].plot.bar(figsize=(10,5),stacked=True,ax=ax)\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_today('industry_funders')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ind_ct = pd.concat([pd.crosstab(df.year,df.ind_top_2)[industries_sorted[:9]],df.loc[[x in industries_sorted[9:] for x in df.ind_top_2]].year.value_counts()],\n",
    "                   axis=1)\n",
    "\n",
    "ind_ct.rename(columns={'year':'other'},inplace=True)\n",
    "\n",
    "ind_ct.rolling(window=3).mean().plot(ax=ax)\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_today('industry_trends')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Who funds what?\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "pd.crosstab(df.ind_top_2,df.disc_top,normalize=0).loc[industries_sorted,:].plot.bar(figsize=(10,5),stacked=True,ax=ax)\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_today('industry_disciplines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SDGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sdg_top_2.isna().sum()/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only get SDG-relevant projects for around 50% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sdg_top_2.value_counts(ascending=True).plot.barh(color='blue',figsize=(8,5))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_today('sdg_counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_sorted = df.sdg_top_2.value_counts(ascending=True).index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SDG funders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(9,6))\n",
    "\n",
    "pd.crosstab(df.sdg_top_2, df.funder,normalize=0).loc[sdg_sorted,funders_sorted].plot.barh(stacked=True,ax=ax)\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_today('sdg_funders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(9,6))\n",
    "\n",
    "pd.crosstab(df.sdg_top_2, df.disc_top,normalize=0).loc[sdg_sorted,disc_sorted].plot.barh(stacked=True,ax=ax)\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_today('sdg_discs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "pd.crosstab(df.ind_top_2, df.sdg_top_2,normalize=0).loc[industries_sorted[::-1],sdg_sorted[::-1]].plot.barh(stacked=True,ax=ax,cmap='tab20c')\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_today('sdg_industries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "pd.crosstab(df.year,df.top_sdg)[sdg_sorted[::-1]].rolling(window=3).mean().plot(ax=ax)\n",
    "\n",
    "# sdg_ct = pd.concat([pd.crosstab(df.year,df.sdg_top_2)[sdg_sorted[:9]],df.loc[[x in sdg_sorted[9:] for x in df.sdg_top_2]].year.value_counts()],\n",
    "#                    axis=1)\n",
    "\n",
    "# sdg_ct.rename(columns={'year':'other'},inplace=True)\n",
    "\n",
    "# sdg_ct.rolling(window=3).mean().plot(ax=ax)\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_today('sdg_trends')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link between enriched data and other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = combined[disc_list+industry_selected+sdg_final].corr()\n",
    "\n",
    "#sims = 1-pairwise_distances(combined[disc_list+industry_selected+sdg_final].T,metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(figsize=(20,20))\n",
    "\n",
    "sns.clustermap(corr_mat,figsize=(18,18),cmap='seismic')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('/Users/jmateosgarcia/Desktop/corr_map.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv(f'../data/processed/{today_str}_combined_gtr_projects.csv',compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
