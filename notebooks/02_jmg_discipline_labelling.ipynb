{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gateway to Research Discipline Labelling\n",
    "\n",
    "We use an older version of GtR with better topic coverage to\n",
    "\n",
    "* Analyse the community structure of the topic co-occurrence network to identify disciplines\n",
    "* Train a discipline classifier that we can use with the latest GtR data.\n",
    "\n",
    "**Observations**\n",
    "\n",
    "During data exploration we have identified a large number of missing abstracts for the social sciences. These abstracts were not missing from previous versions of the data like the one we use here to train our machine learning model. For now, we replace missing abstracts with the older ones but we still need to identify what is the source of the problem - is it GtR or our data collection process?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib\n",
    "#matplotlib.use('cairo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load lda_pipeline.py\n",
    "from gensim import corpora, models\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Characters to drop\n",
    "drop_characters = re.sub('-','',punctuation)+digits\n",
    "\n",
    "#Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('English')\n",
    "\n",
    "#Stem functions\n",
    "from nltk.stem import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def flatten_list(my_list):\n",
    "    '''\n",
    "    Flattens a list\n",
    "    '''\n",
    "    \n",
    "    return([x for el in my_list for x in el])\n",
    "\n",
    "def flatten(my_list):\n",
    "    '''\n",
    "    Flattens a list\n",
    "    '''\n",
    "    \n",
    "    return([x for el in my_list for x in el])\n",
    "\n",
    "\n",
    "def clean_tokenise(string,drop_characters=drop_characters,stopwords=stop):\n",
    "    '''\n",
    "    Takes a string and cleans (makes lowercase and removes stopwords)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "\n",
    "    #Lowercase\n",
    "    str_low = string.lower()\n",
    "    \n",
    "    \n",
    "    #Remove symbols and numbers\n",
    "    str_letters = re.sub('[{drop}]'.format(drop=drop_characters),'',str_low)\n",
    "    \n",
    "    \n",
    "    #Remove stopwords\n",
    "    clean = [x for x in str_letters.split(' ') if (x not in stop) & (x!='')]\n",
    "    \n",
    "    return(clean)\n",
    "\n",
    "\n",
    "class CleanTokenize():\n",
    "    '''\n",
    "    This class takes a list of strings and returns a tokenised, clean list of token lists ready\n",
    "    to be processed with the LdaPipeline\n",
    "    \n",
    "    It has a clean method to remove symbols and stopwords\n",
    "    \n",
    "    It has a bigram method to detect collocated words\n",
    "    \n",
    "    It has a stem method to stem words\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,corpus):\n",
    "        '''\n",
    "        Takes a corpus (list where each element is a string)\n",
    "        '''\n",
    "        \n",
    "        #Store\n",
    "        self.corpus = corpus\n",
    "        \n",
    "    def clean(self,drop=drop_characters,stopwords=stop):\n",
    "        '''\n",
    "        Removes strings and stopwords, \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        cleaned = [clean_tokenise(doc,drop_characters=drop,stopwords=stop) for doc in self.corpus]\n",
    "        \n",
    "        self.tokenised = cleaned\n",
    "        return(self)\n",
    "    \n",
    "    def stem(self):\n",
    "        '''\n",
    "        Optional: stems words\n",
    "        \n",
    "        '''\n",
    "        #Stems each word in each tokenised sentence\n",
    "        stemmed = [[stemmer.stem(word) for word in sentence] for sentence in self.tokenised]\n",
    "    \n",
    "        self.tokenised = stemmed\n",
    "        return(self)\n",
    "        \n",
    "    \n",
    "    def bigram(self,threshold=10):\n",
    "        '''\n",
    "        Optional Create bigrams.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Colocation detector trained on the data\n",
    "        phrases = models.Phrases(self.tokenised,threshold=threshold)\n",
    "        \n",
    "        bigram = models.phrases.Phraser(phrases)\n",
    "        \n",
    "        self.tokenised = bigram[self.tokenised]\n",
    "        \n",
    "        return(self)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "class LdaPipeline():\n",
    "    '''\n",
    "    This class processes lists of keywords.\n",
    "    How does it work?\n",
    "    -It is initialised with a list where every element is a collection of keywords\n",
    "    -It has a method to filter keywords removing those that appear less than a set number of times\n",
    "    \n",
    "    -It has a method to process the filtered df into an object that gensim can work with\n",
    "    -It has a method to train the LDA model with the right parameters\n",
    "    -It has a method to predict the topics in a corpus\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,corpus):\n",
    "        '''\n",
    "        Takes the list of terms\n",
    "        '''\n",
    "        \n",
    "        #Store the corpus\n",
    "        self.tokenised = corpus\n",
    "        \n",
    "    def filter(self,minimum=5):\n",
    "        '''\n",
    "        Removes keywords that appear less than 5 times.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load\n",
    "        tokenised = self.tokenised\n",
    "        \n",
    "        #Count tokens\n",
    "        token_counts = pd.Series([x for el in tokenised for x in el]).value_counts()\n",
    "        \n",
    "        #Tokens to keep\n",
    "        keep = token_counts.index[token_counts>minimum]\n",
    "        \n",
    "        #Filter\n",
    "        tokenised_filtered = [[x for x in el if x in keep] for el in tokenised]\n",
    "        \n",
    "        #Store\n",
    "        self.tokenised = tokenised_filtered\n",
    "        self.empty_groups = np.sum([len(x)==0 for x in tokenised_filtered])\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    def clean(self):\n",
    "        '''\n",
    "        Remove symbols and numbers\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    def process(self):\n",
    "        '''\n",
    "        This creates the bag of words we use in the gensim analysis\n",
    "        \n",
    "        '''\n",
    "        #Load the list of keywords\n",
    "        tokenised = self.tokenised\n",
    "        \n",
    "        #Create the dictionary\n",
    "        dictionary = corpora.Dictionary(tokenised)\n",
    "        \n",
    "        #Create the Bag of words. This converts keywords into ids\n",
    "        corpus = [dictionary.doc2bow(x) for x in tokenised]\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        self.dictionary = dictionary\n",
    "        return(self)\n",
    "        \n",
    "    def tfidf(self):\n",
    "        '''\n",
    "        This is optional: We extract the term-frequency inverse document frequency of the words in\n",
    "        the corpus. The idea is to identify those keywords that are more salient in a document by normalising over\n",
    "        their frequency in the whole corpus\n",
    "        \n",
    "        '''\n",
    "        #Load the corpus\n",
    "        corpus = self.corpus\n",
    "        \n",
    "        #Fit a TFIDF model on the data\n",
    "        tfidf = models.TfidfModel(corpus)\n",
    "        \n",
    "        #Transform the corpus and save it\n",
    "        self.corpus = tfidf[corpus]\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    def fit_lda(self,num_topics=20,passes=5,iterations=75,random_state=1803):\n",
    "        '''\n",
    "        \n",
    "        This fits the LDA model taking a set of keyword arguments.\n",
    "        #Number of passes, iterations and random state for reproducibility. We will have to consider\n",
    "        reproducibility eventually.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load the corpus\n",
    "        corpus = self.corpus\n",
    "        \n",
    "        #Train the LDA model with the parameters we supplied\n",
    "        lda = models.LdaModel(corpus,id2word=self.dictionary,\n",
    "                              num_topics=num_topics,passes=passes,iterations=iterations,random_state=random_state)\n",
    "        \n",
    "        #Save the outputs\n",
    "        self.lda_model = lda\n",
    "        self.lda_topics = lda.show_topics(num_topics=num_topics)\n",
    "        \n",
    "\n",
    "        return(self)\n",
    "    \n",
    "    def predict_topics(self):\n",
    "        '''\n",
    "        This predicts the topic mix for every observation in the corpus\n",
    "        \n",
    "        '''\n",
    "        #Load the attributes we will be working with\n",
    "        lda = self.lda_model\n",
    "        corpus = self.corpus\n",
    "        \n",
    "        #Now we create a df\n",
    "        predicted = lda[corpus]\n",
    "        \n",
    "        #Convert this into a dataframe\n",
    "        predicted_df = pd.concat([pd.DataFrame({x[0]:x[1] for x in topics},\n",
    "                                              index=[num]) for num,topics in enumerate(predicted)]).fillna(0)\n",
    "        \n",
    "        self.predicted_df = predicted_df\n",
    "        \n",
    "        return(self)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "\n",
    "We are using as an input the processed data from `01`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed('8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_gtr_df = pd.read_csv(\n",
    "    #'../data/raw/gtr/gtr_projects.csv',\n",
    "    '../data/processed/13_6_2019_gtr_for_prediction.csv',\n",
    "    converters={\n",
    "        'research_topics': ast.literal_eval,\n",
    "        'researc_subjects': ast.literal_eval,\n",
    "    }\n",
    ")\n",
    "\n",
    "# gtr_df = raw_gtr_df[(raw_gtr_df['start_year'] >= 2006) & (raw_gtr_df['start_year'] < 2017)]\n",
    "# gtr_df = gtr_df[(gtr_df['funder_name'] != 'BBSRC') & (gtr_df['funder_name'] != 'MRC')]\n",
    "\n",
    "gtr_df = raw_gtr_df[(raw_gtr_df['year'] >= 2006) & (raw_gtr_df['year'] < 2019)]\n",
    "#gtr_df = gtr_df[(gtr_df['leadFunder'] != 'BBSRC') & (gtr_df['leadFunder'] != 'MRC')]\n",
    "\n",
    "#gtr_df = gtr_df[(gtr_df['leadFunder'] != 'MRC')]\n",
    "\n",
    "gtr_df = gtr_df.loc[['Unclassified' not in x for x in gtr_df['research_topics']]]\n",
    "\n",
    "gtr_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify topic communities in the period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create all years co-occurrence graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import networkx as nx\n",
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create an edgelist from the groups in each project\n",
    "\n",
    "#This creates combinations \n",
    "combs = [sorted(tuple) for tuple in flatten([list(itertools.combinations(vals,2)) for vals in gtr_df['research_topics']])]\n",
    "\n",
    "#Count combinations (this is the weight)\n",
    "combs_df = pd.Series(['__'.join(x) for x in combs]).value_counts().reset_index(drop=False)\n",
    "\n",
    "combs_df.columns = ['vars','weight']\n",
    "\n",
    "#This is an edgelist\n",
    "combs_df['source'],combs_df['target'] = [[v.split('__')[num] for v in combs_df['vars']] for num in [0,1]]\n",
    "\n",
    "combs_df.drop(['vars'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a network\n",
    "network = nx.from_pandas_edgelist(combs_df,edge_attr='weight')\n",
    "\n",
    "#Extract the best partition\n",
    "part = community.best_partition(network,resolution=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB This is only a preliminary analysis that we still need to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What do these look like?\n",
    "pd.Series(part).reset_index(drop=False).groupby(0)['index'].apply(lambda x: print(' '.join(list(x))+'\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The communities look intuitive. We will create a lookup and convert each topic into its discipline.\n",
    "\n",
    "We will assign projects to their top discipline distinguishing between 'pure' discipline projects and mixed ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_name_lookup = {6:'biological_sciences',\n",
    "                        1:'physics',\n",
    "                        0:'engineering_technology',\n",
    "                        2:'environmental_sciences',\n",
    "                        4:'social_sciences',\n",
    "                        3:'arts_humanities',\n",
    "                       5:'mathematics_computing'}\n",
    "\n",
    "\n",
    "topic_discipline_lookup = {top:category_name_lookup[disc] for top,disc in part.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look-up the disciplines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_df['discipline'] = gtr_df['research_topics'].apply(lambda x: [topic_discipline_lookup[val] for val in x])\n",
    "\n",
    "gtr_df['discipline'] = [['medical_sciences'] if lf=='MRC' else x for x,lf in zip(gtr_df['discipline'],gtr_df['leadFunder'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_df['discipline_sets'] = [set(x) for x in gtr_df['discipline']]\n",
    "\n",
    "gtr_df['single_disc'] = [True if len(x)==1 else np.nan if len(x)==0 else False for x in gtr_df['discipline_sets']]\n",
    "\n",
    "gtr_df['single_disc'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "92% of projects are pure discipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_df.loc[gtr_df.single_disc==True,'discipline_sets'].value_counts().plot.bar(color='darkblue',title='Disciplines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the absence of medical sciences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we very crudely assume that any project funded by the MRC is in 'Health'\n",
    "\n",
    "gtr_df['discipline_sets'] = [set(['medical_sciences']) if f =='MRC' else x for f,x in zip(gtr_df['leadFunder'],\n",
    "                                                                                       gtr_df['discipline_sets'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And now we create the training set\n",
    "\n",
    "#Also dropping the cases with no abstracts\n",
    "gtr_pure = gtr_df.loc[[len(x)==1 for x in gtr_df['discipline_sets']]].dropna(axis=0,subset=['abstractText'])\n",
    "\n",
    "gtr_pure.leadFunder.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML imports\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score,GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore',UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def flatten_list(my_list):\n",
    "    '''\n",
    "    Flattens a list\n",
    "    '''\n",
    "    \n",
    "    return([x for el in my_list for x in el])\n",
    "\n",
    "\n",
    "def dummies_from_list(list_of_categories):\n",
    "    '''\n",
    "    This function takes a list of categories and returns a df where every column is a dummie for each unique variable\n",
    "    in the category. Admittedly, the function could be nicer.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    #We concatenate a bunch of series whose indices are the names of the variables.\n",
    "    #We could have done something similar by creating DFs with one row\n",
    "    \n",
    "    cats = [x for x in set(flatten_list(list_of_categories))]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for category in cats:\n",
    "    \n",
    "        var = [category in x for x in list_of_categories]\n",
    "\n",
    "        df[category] = var\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #dummy_df = pd.concat([pd.Series({v:1 for v in obs}) for obs in list_of_categories],axis=1).T.fillna(0)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load text_classifier.py\n",
    "# CLasses\n",
    "\n",
    "#One class for text classification based on text inputs\n",
    "\n",
    "class TextClassification():\n",
    "    '''\n",
    "    This class takes a corpus (could be a list of strings or a tokenised corpus) and a target (could be multiclass or single class).\n",
    "    \n",
    "    When it is initialised it vectorises the list of tokens using sklearn's count vectoriser.\n",
    "    \n",
    "    It has a grid search method that takes a list of models and parameters and trains the model.\n",
    "    \n",
    "    It returns the output of grid search for diagnosis\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,corpus,target):\n",
    "        '''\n",
    "        \n",
    "        Initialise. The class will recognise if we are feeding it a list of strings or a list of\n",
    "        tokenised documents and vectorise accordingly. \n",
    "        \n",
    "        It will also recognise is this a multiclass or one class problem based on the dimensions of the target array\n",
    "        \n",
    "        Later on, it will use control flow to modify model parameters depending on the type of data we have\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Is this a multiclass classification problem or a single class classification problem?\n",
    "        if target.shape[1]>1:\n",
    "            self.mode = 'multiclass'\n",
    "            \n",
    "        else:\n",
    "            self.mode = 'single_class'\n",
    "    \n",
    "    \n",
    "        #Store the target\n",
    "        self.Y = target\n",
    "    \n",
    "        #Did we feed the model a bunch of strings or a list of tokenised docs? If the latter, we clean and tokenise.\n",
    "        \n",
    "        if type(corpus[0])==str:\n",
    "            #corpus = CleanTokenize(corpus).clean().bigram().tokenised\n",
    "            corpus = CleanTokenize(corpus).clean().tokenised\n",
    "            \n",
    "        #Turn every list of tokens into a string for count vectorising\n",
    "        corpus_string =  [' '.join(words) for words in corpus]\n",
    "        \n",
    "        \n",
    "        #And then we count vectorise in a hacky way.\n",
    "        count_vect = CountVectorizer(stop_words='english',min_df=5).fit(corpus_string)\n",
    "        \n",
    "        #Store the features\n",
    "        self.X = count_vect.transform(corpus_string)\n",
    "        \n",
    "        #Store the count vectoriser (we will use it later on for prediction on new data)\n",
    "        self.count_vect = count_vect\n",
    "        \n",
    "    def grid_search(self,models):\n",
    "        '''\n",
    "        The grid search method takes a list with models and their parameters and it does grid search crossvalidation.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load inputs and targets into the model\n",
    "        Y = self.Y\n",
    "        X = self.X\n",
    "        \n",
    "        if self.mode=='multiclass':\n",
    "            '''\n",
    "            If the model is multiclass then we need to add some prefixes to the model paramas\n",
    "            \n",
    "            '''\n",
    "        \n",
    "            for mod in models:\n",
    "                #Make ovr\n",
    "                mod[0] = OneVsRestClassifier(mod[0])\n",
    "                \n",
    "                #Add the estimator prefix\n",
    "                mod[1] = {'estimator__'+k:v for k,v in mod[1].items()}\n",
    "                \n",
    "        \n",
    "        #Container with results\n",
    "        results = []\n",
    "\n",
    "        #For each model, run the analysis.\n",
    "        for num,mod in enumerate(models):\n",
    "            print(num)\n",
    "\n",
    "            #Run the classifier\n",
    "            clf = GridSearchCV(mod[0],mod[1])\n",
    "\n",
    "            #Fit\n",
    "            clf.fit(X,Y)\n",
    "\n",
    "            #Append results\n",
    "            results.append(clf)\n",
    "        \n",
    "        self.results = results\n",
    "        return(self)\n",
    "\n",
    "    \n",
    "#Class to visualise the outputs of multilabel models.\n",
    "\n",
    "#I call it OrangeBrick after YellowBrick, the package for ML output visualisation \n",
    "#(which currently doesn't support multilabel classification)\n",
    "\n",
    "\n",
    "class OrangeBrick():\n",
    "    '''\n",
    "    This class takes a df with the true classes for a multilabel classification exercise and produces some charts visualising findings.\n",
    "    \n",
    "    The methods include:\n",
    "    \n",
    "        .confusion_stack: creates a stacked barchart with the confusion matrices stacked by category, sorting classes by performance\n",
    "        .prec_rec: creates a barchart showing each class precision and recall;\n",
    "        #Tobe done: Consider mixes between classes?\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,true_labels,predicted_labels,var_names):\n",
    "        '''\n",
    "        Initialise with a true labels, predicted labels and the variable names\n",
    "        '''\n",
    "         \n",
    "        self.true_labels = true_labels\n",
    "        self.predicted_labels = predicted_labels\n",
    "        self.var_names = var_names\n",
    "    \n",
    "    def make_metrics(self):\n",
    "        '''\n",
    "        Estimates performance metrics (for now just confusion charts by class and precision/recall scores for the 0.5 \n",
    "        decision rule.\n",
    "        \n",
    "        '''\n",
    "        #NB in a confusion matrix in SKlearn the X axis indicates the predicted class and the Y axis indicates the ground truth.\n",
    "        #This means that:\n",
    "            #cf[0,0]-> TN\n",
    "            #cf[1,1]-> TP\n",
    "            #cf[0,1]-> FN (prediction is false, groundtruth is true)\n",
    "            #cf[1,0]-> FP (prediction is true, ground truth is false)\n",
    "\n",
    "\n",
    "\n",
    "        #Predictions and true labels\n",
    "        true_labels = self.true_labels\n",
    "        pred_labels = self.predicted_labels\n",
    "\n",
    "        #Variable names\n",
    "        var_names = self.var_names\n",
    "\n",
    "        #Store confusion matrices\n",
    "        score_store = []\n",
    "\n",
    "\n",
    "        for num in np.arange(len(var_names)):\n",
    "\n",
    "            #This is the confusion matrix\n",
    "            cf = confusion_matrix(pred_labels[:,num],true_labels[:,num])\n",
    "\n",
    "            #This is a melted confusion matrix\n",
    "            melt_cf = pd.melt(pd.DataFrame(cf).reset_index(drop=False),id_vars='index')['value']\n",
    "            melt_cf.index = ['true_negative','false_positive','false_negative','true_positive']\n",
    "            melt_cf.name = var_names[num]\n",
    "            \n",
    "            #Order variables to separate failed vs correct predictions\n",
    "            melt_cf = melt_cf.loc[['true_positive','true_negative','false_positive','false_negative']]\n",
    "\n",
    "            #We are also interested in precision and recall\n",
    "            prec = cf[1,1]/(cf[1,1]+cf[1,0])\n",
    "            rec = cf[1,1]/(cf[1,1]+cf[0,1])\n",
    "\n",
    "            prec_rec = pd.Series([prec,rec],index=['precision','recall'])\n",
    "            prec_rec.name = var_names[num]\n",
    "            score_store.append([melt_cf,prec_rec])\n",
    "    \n",
    "        self.score_store = score_store\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    def confusion_chart(self,ax):\n",
    "        '''\n",
    "        Plot the confusion charts\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Visualise confusion matrix outputs\n",
    "        cf_df = pd.concat([x[0] for x in self.score_store],1)\n",
    "\n",
    "        #This ranks categories by the error rates\n",
    "        failure_rate = cf_df.apply(lambda x: x/x.sum(),axis=0).loc[['false' in x for x in cf_df.index]].sum().sort_values(\n",
    "            ascending=False).index\n",
    "\n",
    "        \n",
    "        #Plot and add labels\n",
    "        cf_df.T.loc[failure_rate,:].plot.bar(stacked=True,ax=ax,width=0.8,cmap='Accent')\n",
    "\n",
    "        ax.legend(bbox_to_anchor=(1.01,1))\n",
    "        #ax.set_title('Stacked confusion matrix for disease areas',size=16)\n",
    "    \n",
    "    \n",
    "    def prec_rec_chart(self,ax):\n",
    "        '''\n",
    "        \n",
    "        Plot a precision-recall chart\n",
    "        \n",
    "        '''\n",
    "    \n",
    "\n",
    "        #Again, we sort them here to assess model performance in different disease areas\n",
    "        prec_rec = pd.concat([x[1] for x in self.score_store],1).T.sort_values('precision')\n",
    "        prec_rec.plot.bar(ax=ax)\n",
    "\n",
    "        #Add legend and title\n",
    "        ax.legend(bbox_to_anchor=(1.01,1))\n",
    "        #ax.set_title('Precision and Recall by disease area',size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is the corpus\n",
    "corpus = list(gtr_pure['abstractText'])\n",
    "\n",
    "#We use a utility function to create a df for a one vs rest classification\n",
    "target = dummies_from_list(gtr_pure['discipline_sets'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run grid search with these model parameters\n",
    "my_models = [\n",
    "    [RandomForestClassifier(),\n",
    "     {'class_weight':['balanced',None],'min_samples_leaf':[1,5]}],\n",
    "    \n",
    "    [LogisticRegression(),\n",
    "     {'class_weight':['balanced',None],'penalty':['l1','l2'],\n",
    "      'C':[0.1,1,100]}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict groups\n",
    "\n",
    "#Initialise the TextClassification class\n",
    "gtr_t = TextClassification(corpus,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_t.grid_search(my_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check scores and best estimators\n",
    "for res in gtr_t.results:\n",
    "    print(res.best_score_)\n",
    "    print(res.best_estimator_)\n",
    "    \n",
    "    #This is the best estimator\n",
    "best_est = gtr_t.results[1].best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_diag = OrangeBrick(true_labels=np.array(target),\n",
    "                      predicted_labels=best_est.predict(gtr_t.X),\n",
    "                      var_names=target.columns).make_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=2,figsize=(10,7.5))\n",
    "\n",
    "gtr_diag.confusion_chart(ax=ax[0])\n",
    "gtr_diag.prec_rec_chart(ax=ax[1])\n",
    "\n",
    "#fig.suptitle('Model evaluation for GTR disciplines',y=1.01,size=16)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load unlabelled data for prediction\n",
    "\n",
    "We will label all the projects in the df we loaded initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_file(date_list,date_format='%d-%m-%Y'):\n",
    "    '''\n",
    "    This function takes a list of date strings and returns the most recent one\n",
    "    \n",
    "    Args:\n",
    "        date_list: a list of strings with the format date_filename\n",
    "        date_format: the format for the date, defaults to %d-%m-%Y\n",
    "    \n",
    "    Returns:\n",
    "        The element with the latest date\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #This gets the maximum date in the gtr directory\n",
    "    dates = [datetime.datetime.strptime('-'.join(x.split('_')[:3]),date_format) for x in date_list]\n",
    "    \n",
    "    #Return the most recent file\n",
    "    most_recent = sorted([(x,y) for x,y in zip(date_list,dates)],key=lambda x:x[1])[-1][0]\n",
    "    \n",
    "    return(most_recent)\n",
    "                                        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_files = [x for x in os.listdir('../data/processed/') if 'for_prediction' in x]\n",
    "\n",
    "\n",
    "latest_file = get_latest_file(prediction_files)\n",
    "\n",
    "latest_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gtr_unlabelled = pd.read_csv('../data/processed/'+latest_file)\n",
    "\n",
    "gtr_unlabelled = raw_gtr_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During EDA we have noticed several garbagey abstract names. Let's weed them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beginning_abs = pd.Series([x[:100] for x in gtr_unlabelled['abstractText']]).value_counts()\n",
    "\n",
    "beginning_abs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issue: Missing abstracts in 2010 ESRC data. Are they present in the older dataset / can we replace them from there?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_abstracts_esrc = gtr_unlabelled.query('leadFunder == \"ESRC\" & year < 2014')['abstractText']\n",
    "missing_abstracts_esrc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_abstracts_esrc_indices = list(missing_abstracts_esrc.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_gtr_df = pd.read_csv(\n",
    "    '../data/raw/gtr/gtr_projects.csv',\n",
    "    #'../data/processed/13_6_2019_gtr_for_prediction.csv',\n",
    "    converters={\n",
    "        'research_topics': ast.literal_eval,\n",
    "        'researc_subjects': ast.literal_eval,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_gtr_df['project_id_short'] = old_gtr_df['project_id'].apply(lambda x: x.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "older_abstracts = {index:abstract for index,abstract in zip(old_gtr_df['project_id_short'],\n",
    "                                                            old_gtr_df['abstract_texts'])  if (index in missing_abstracts_esrc_indices) &\n",
    "                  (pd.isnull(abstract)==False)}\n",
    "\n",
    "#Most of these projects actually have abstracts - odd!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_unlabelled['missing_abs'] = [abst[:10] in beginning_abs.index[0] for abst in gtr_unlabelled['abstractText']]\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "pd.crosstab(gtr_unlabelled['year'],gtr_unlabelled['missing_abs'],normalize=1).plot(ax=ax)\n",
    "\n",
    "ax.set_xlim(2006,2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we replace these in the unlabelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_unlabelled['abstractText'] = [abst if ind not in older_abstracts.keys() else older_abstracts[ind] for\n",
    "                                         ind,abst in zip(gtr_unlabelled.index,\n",
    "                                                            gtr_unlabelled['abstractText'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we redo the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now we will drop any project with abstract appearing > 25 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uninformative_abstracts = list(beginning_abs.index[beginning_abs>25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_unlabelled_cleaned = gtr_unlabelled.loc[[x[:100] not in uninformative_abstracts for x in gtr_unlabelled['abstractText']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_unlab_features = gtr_t.count_vect.transform(gtr_unlabelled_cleaned['abstractText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicted labels\n",
    "gtr_unlab_probs = pd.DataFrame(best_est.predict_proba(gtr_unlab_features),columns=target.columns)\n",
    "\n",
    "#Get discipline names to subset things easily later\n",
    "discs = target.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_predicted = pd.concat([gtr_unlabelled_cleaned.reset_index(drop=False),gtr_unlab_probs],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../models/{today_str}_gtr_discipline_classifier.p','wb') as outfile:\n",
    "    pickle.dump(gtr_t,outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What disciplines co-occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(gtr_predicted[discs].applymap(lambda x: 1 if x>0.1 else 0).corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Who funds what disciplines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(5,7))\n",
    "\n",
    "gtr_predicted.groupby(['leadFunder'])[discs].mean().plot.barh(ax=ax,width=0.8,title='Mean predicted probabilities by funder')\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Makes sense. Each funder seems to focus on its 'core discipline'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the trends over times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "pd.crosstab(gtr_predicted['year'],gtr_predicted[discs].idxmax(axis=1)).plot(ax=ax)\n",
    "\n",
    "ax.set_xlim(2006,2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about the grant type category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(5,9))\n",
    "\n",
    "gtr_predicted.groupby(['grantCategory'])[discs].mean().plot.barh(ax=ax,width=0.8,title='Mean predicted probabilities by grant category')\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important differences here. For example...\n",
    "\n",
    "* Procurement is dominated by maths. Is this because it picks up IT procuremenr?\n",
    "* Proof of market / concept projects dominated by engineering and technology.\n",
    "* Vouchers / SIGs / SME support have a strong presence of physics, picking up access to facilities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tidy up the variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_clean = ['id_y','title','abstractText','year',\n",
    "                 'leadFunder','status','grantCategory','amount','currencyCode',\n",
    "                 'prods','tech','spin','pubs','databases',\n",
    "                 'research_topics','research_activities',\n",
    "                 'mathematics_computing','engineering_technology','environmental_sciences','social_sciences','physics',\n",
    "                 'medical_sciences','biological_sciences','arts_humanities']\n",
    "gtr_predicted = gtr_predicted[columns_clean]\n",
    "\n",
    "gtr_new = gtr_predicted.copy()\n",
    "\n",
    "gtr_new.rename(columns={'id_y':'project_id','abstractText':'abstract',\n",
    "                             'leadFunder':'funder','grantCategory':'grant_category','currencyCode':'currency',\n",
    "                             'prods':'out_prod','tech':'out_tech','spin':'out_spin','pubs':'out_pubs','databases':'out_db',\n",
    "                              'mathematics_computing':'disc_maths_comp',\n",
    "                              'engineering_technology':'disc_eng_tech',\n",
    "                              'environmental_sciences':'disc_env',\n",
    "                              'social_sciences':'disc_social',\n",
    "                              'physics':'disc_physics',\n",
    "                              'medical_sciences':'disc_medical',\n",
    "                              'biological_sciences':'disc_biological',\n",
    "                              'arts_humanities':'disc_arts_humanities'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_new.to_csv(f'../data/processed/{today_str}_gtr_labelled.csv',compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_check(corpus,num,length):\n",
    "    '''\n",
    "    Prints num random examples form corpus\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    selected = np.random.randint(0,len(corpus),num)\n",
    "    \n",
    "    texts  = [text for num,text in enumerate(corpus) if num in selected]\n",
    "    \n",
    "    for t in texts:\n",
    "        print(t[:length])\n",
    "        print('====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
