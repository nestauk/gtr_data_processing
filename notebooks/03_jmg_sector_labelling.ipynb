{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sector labelling\n",
    "\n",
    "Here we use an industrial classifier trained on business website data to predict the sector of GtR projects.\n",
    "\n",
    "We also label the data with a couple of variables (prediction percentile and prediction tightness that should help evaluate the quality of the classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% run notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put functions and things here\n",
    "\n",
    "def get_latest_file(date_list,date_format='%d-%m-%Y'):\n",
    "    '''\n",
    "    This function takes a list of date strings and returns the most recent one\n",
    "    \n",
    "    Args:\n",
    "        date_list: a list of strings with the format date_filename\n",
    "        date_format: the format for the date, defaults to %d-%m-%Y\n",
    "    \n",
    "    Returns:\n",
    "        The element with the latest date\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #This gets the maximum date in the gtr directory\n",
    "    dates = [datetime.datetime.strptime('-'.join(x.split('_')[:3]),date_format) for x in date_list]\n",
    "    \n",
    "    #Return the most recent file\n",
    "    most_recent = sorted([(x,y) for x,y in zip(date_list,dates)],key=lambda x:x[1])[-1][0]\n",
    "    \n",
    "    return(most_recent)\n",
    "                                   \n",
    "\n",
    "def flatten_list(my_list):\n",
    "    '''\n",
    "    Flattens a list\n",
    "    '''\n",
    "    \n",
    "    return([x for el in my_list for x in el])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_files = [x for x in os.listdir('../data/processed/') if 'labelled' in x]\n",
    "\n",
    "\n",
    "# latest_file = get_latest_file(prediction_files)\n",
    "\n",
    "# latest_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelled data\n",
    "\n",
    "Note that we are working on a subset of the GtR data that removes projects with garbagey abstracts (see `02_jmg_discipline_labelling`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gtr_labelled = pd.read_csv('../data/processed/'+latest_file,compression='zip')\n",
    "\n",
    "gtr_labelled = pd.read_csv('../data/processed/2_5_2019_gtr_labelled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_labelled.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_predictions = pd.read_csv('../data/processed/gtr_abstracts_industries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_predictions.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_labelled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert gtr_labelled.shape[0] == sector_predictions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to make sure we are matching the right labels with the right industry predictions\n",
    "gtr_w_industries = pd.concat([gtr_labelled,sector_predictions],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_names = sector_predictions.columns[2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_w_industries['top_industry'] = gtr_w_industries[industry_names].idxmax(axis=1)\n",
    "\n",
    "gtr_w_industries['top_industry'].value_counts().plot.bar(color='blue',figsize=(10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_w_industries.to_csv(f'../data/processed/{today_str}_gtr_with_industry_labels.csv',compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Measures of quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking for tight predictions\n",
    "\n",
    "We are particularly interested in predictions that are 'tight' (ie the distribution is highly skewed) and confident (they have high values)\n",
    "\n",
    "We do this a couple of ways\n",
    "\n",
    "1. Calculate variance in prediction for each observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_w_industries['industry_pred_var'] = gtr_w_industries[industry_names].apply(lambda x: np.var(x),axis=1)\n",
    "\n",
    "industry_sorted_variance = gtr_w_industries.groupby('top_industry')['industry_pred_var'].mean().sort_values(ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_data = [[x for x,y in zip(gtr_w_industries['industry_pred_var'],gtr_w_industries['top_industry']) if y== ind] for ind in industry_sorted_variance]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "ax.boxplot(boxplot_data)\n",
    "\n",
    "ax.set_xticklabels(industry_sorted_variance,rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_variance_quartile = gtr_w_industries.groupby('top_industry')['industry_pred_var'].apply(lambda x: np.percentile(x,75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_w_industries['tight_prediction'] = [x>pred_variance_quartile[sector] for x,sector in zip(gtr_w_industries['industry_pred_var'],gtr_w_industries['top_industry'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate kurtosis for each prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_w_industries['kurtosis'] = gtr_w_industries[industry_names].apply(lambda x: kurtosis(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_w_industries.groupby('top_industry')['kurtosis'].mean().sort_values(ascending=False).plot.bar(figsize=(10,5),color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_kurtosis = np.percentile(gtr_w_industries['kurtosis'],50)\n",
    "\n",
    "gtr_w_industries['above_median_kurtosis'] = gtr_w_industries['kurtosis']>median_kurtosis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify more highly confident predictions in total and by sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_75_preds_all = np.percentile(flatten_list([gtr_w_industries[sector] for sector in industry_names]),75)\n",
    "\n",
    "#Are any of the predictions for a project above the 75 pc for all predictions?\n",
    "\n",
    "gtr_w_industries['has_top_pred']= gtr_w_industries[industry_names].apply(lambda x: any(v>pc_75_preds_all for v in x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_75_by_sector = {sector: np.percentile(gtr_w_industries[sector],75) for sector in industry_names}\n",
    "\n",
    "pd.DataFrame(pc_75_by_sector,index=[0]).T.sort_values(0,ascending=False).plot.bar(figsize=(10,5),color='blue',legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sector in industry_names:\n",
    "    \n",
    "    gtr_w_industries[sector+'_top_q'] = gtr_w_industries[sector]>pc_75_by_sector[sector]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove some sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After some manual checking, we remove the below. They tend to misclassify projects for a variety of reasons potentially linked to noise in the source data\n",
    "\n",
    "sectors_remove = ['services_consumer_retail','services_education_post_primary','services_travelling','services_real_state','services_administrative',\n",
    "                 'services_electronics_machinery','primary_fishing','services_utilities)']\n",
    "\n",
    "industry_selected = [x for x in industry_names if x not in sectors_remove]\n",
    "\n",
    "gtr_w_industries['top_industry_2'] = gtr_w_industries[industry_selected].idxmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_w_industries.to_csv(f'../data/processed/{today_str}_gtr_w_industries.csv',compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: salient word extraction\n",
    "\n",
    "Here we want to extract salient words from groups of data in order to visually interpret results.\n",
    "\n",
    "We will create a function that groups the data into aggregated corpora by categories of interest, generates counts and normalises.\n",
    "\n",
    "It returns a dict with words that can also be visualised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string as st\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalientWords():\n",
    "    '''\n",
    "    Class that extracts salient words from clusters of data.\n",
    "    \n",
    "    Arguments:\n",
    "        A dataframe and two strings (the variable to groupby and the variable to use as text)\n",
    "        \n",
    "    Methods:\n",
    "        .count_vect(): word frequencies for all words (takes **kwargs for additional parameters in the count vectorisation)\n",
    "        .salient(): tfidf. It will also take **kwargs and a threshold for including words in the results\n",
    "        .visualise(): it visualises the data as wordclouds\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,df,categories,text):\n",
    "        '''\n",
    "        Initialises with key variables\n",
    "        \n",
    "        '''    \n",
    "        \n",
    "        \n",
    "        #This creates the joined corpus\n",
    "        self.grouped_corpus = df.groupby(categories)[text].apply(lambda x: ' '.join(x))\n",
    "        \n",
    "        \n",
    "        #Remove digits and special \n",
    "        dig = r'|'.join(st.digits)\n",
    "        out = '\\W+'\n",
    "        \n",
    "        self.processed_text = [re.sub(out,' ',re.sub(dig,' ',x.lower())) for x in self.grouped_corpus]\n",
    "        \n",
    "        #This is a dict we will use to store the results later\n",
    "        self.groups = {i:[] for i in self.grouped_corpus.index}\n",
    "        \n",
    "        #return(self)\n",
    "        \n",
    "    def word_freqs(self,**kwargs):\n",
    "        '''\n",
    "        Terms frequencies over categories\n",
    "        \n",
    "        '''\n",
    "        #load corpus\n",
    "        X = self.processed_text\n",
    "        \n",
    "        count_vect = CountVectorizer(**kwargs)\n",
    "        \n",
    "        #Store outputs\n",
    "        self.count_vect = count_vect\n",
    "        self.token_freqs = count_vect.fit_transform(X)\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    def salient(self,min_threshold=1000,extra_stops=['research','project','new','projects'],**kwargs):\n",
    "        '''\n",
    "        Salient terms in the data.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Get selected words\n",
    "        \n",
    "        word_freqs = pd.DataFrame(self.token_freqs.todense(),columns=self.count_vect.get_feature_names())\n",
    "        \n",
    "        word_freqs_total = word_freqs.sum(axis=0)\n",
    "        \n",
    "        #Create a dict so we can put word frequencies together with salient words later\n",
    "        #word_freqs_dict = word_freqs.to_dict()\n",
    "        \n",
    "        #I am interested in any words above the threshold\n",
    "        my_words = [x for x in word_freqs_total.index[word_freqs_total>min_threshold] if x not in extra_stops]\n",
    "        \n",
    "        \n",
    "        #Initialise the tfidf\n",
    "        tf = TfidfTransformer(**kwargs)\n",
    "        \n",
    "        \n",
    "        #out\n",
    "        X = tf.fit_transform(self.token_freqs)\n",
    "        \n",
    "        X_selected = pd.DataFrame(X.todense(),columns=self.count_vect.get_feature_names())[my_words]\n",
    "            \n",
    "            \n",
    "        #Store the results\n",
    "        for n,x in enumerate(self.groups.keys()):\n",
    "            \n",
    "            #Creates the dataframe combining tfs and wfs\n",
    "            result = pd.concat([X_selected.iloc[n],word_freqs.iloc[n][my_words]],axis=1)\n",
    "            \n",
    "            result.columns = ['tfidf','count'] \n",
    "                   \n",
    "            self.groups[x] = result\n",
    "            \n",
    "        return(self)\n",
    "        \n",
    "    def get_summary(self,tf_threshold=90,wf_threshold=75):\n",
    "        '''\n",
    "        \n",
    "        Extracts a summary of the data based on tf and wf thresholds\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.summary={i:[] for i in self.groups.keys()}\n",
    "        \n",
    "        for x in self.groups.keys():\n",
    "            \n",
    "            #Creates the dataframe\n",
    "            result = self.groups[x]\n",
    "            \n",
    "            tf_thres = np.percentile(result['tfidf'],tf_threshold)\n",
    "            \n",
    "            summary = result.loc[result['tfidf']>tf_thres]\n",
    "            \n",
    "            wf_thres = np.percentile(result['count'],wf_threshold)\n",
    "            \n",
    "            summary_2 = summary.loc[summary['count']>wf_thres]\n",
    "                   \n",
    "            self.summary[x] = summary_2.sort_values('tfidf',ascending=False)\n",
    "        \n",
    "        return(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal = SalientWords(gtr_w_industries,categories='top_industry_2',text='abstract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal.word_freqs(**{'stop_words':'english','max_features':20000,'ngram_range':(1,2)}).salient(min_threshold=500).get_summary(wf_threshold=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wordcloud(term_freqs_df,var,name,ax):\n",
    "    '''\n",
    "    This function takes a df generated by the SalientWords class and returns a wordcloud\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    input_dict = {w:f for w,f in zip(term_freqs_df.index,term_freqs_df[var])}\n",
    "\n",
    "    wc = wordcloud.WordCloud(background_color=\"black\").generate_from_frequencies(input_dict)\n",
    "\n",
    "    ax.imshow(wc)\n",
    "    \n",
    "    ax.axis('off')\n",
    "    \n",
    "    ax.set_title(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=2,nrows=20,figsize=(10,40))\n",
    "\n",
    "for n,name in enumerate(sal.summary.keys()):\n",
    "    \n",
    "    #print(n)\n",
    "    \n",
    "    if n<20:\n",
    "        make_wordcloud(sal.summary[name],'tfidf',name,ax=ax[n][0])\n",
    "        \n",
    "    else:\n",
    "        make_wordcloud(sal.summary[name],'tfidf',name,ax=ax[n-20][1])\n",
    "        \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
